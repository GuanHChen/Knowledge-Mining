{"title":"Knowledge Mining: Lab04 R programming (Unsupervised learning)","markdown":{"yaml":{"title":"Knowledge Mining: Lab04 R programming (Unsupervised learning)","author":"Guan Chen","output":{"html_document":{"toc":"yes","toc_float":"yes","highlight":"default","theme":"libera","number_sections":"yes"}},"date":"last-modified","title-block-banner":true},"headingText":"Unsupervised Learning","containsRefs":false,"markdown":"\n\n\nUnsupervised learning is a class of machine learning algorithms to identify patterns or grouping structure in the data. Unlike supervised learning which relies on \"supervised\" information such as the dependent variable to guide modeling, unsupervised learning seeks to explore the structure and possible groupings of unlabeled data. This information will be useful to provide pre-processor for supervised learning.\n\nUnsupervised learning has no explicit dependent variable of Y for prediction. Instead, the goal is to discover interesting patterns about the measurements on $(X_{1}), (X_{2}), . . . , (X_{p})$ and identify any subgroups among the observations.\n\nGenerally, in this section, the two general methods are introduced: Principal components analysis and Clustering.\n\n## Principal Component Analysis (PCA)\n\nPrincipal Components Analysis (PCA) produces a low-dimensional representation of a dataset. It finds a sequence of linear combinations of the variables that have maximal variance, and are mutually uncorrelated.\n\nThe first principal component of a set of features $(X_1, X_2, . . . , X_p)$ is the normalized linear combination of the features: <br> $$  Z_1 = \\phi_{11}X_1 +\\phi_{21}X_2 +...+\\phi_{p1}X_p $$ <br>\n\nthat has the largest variance. By normalized, we mean that $\\sum_{j=1}^p\\phi_{j1}^2 = 1$.\n\nThe elements $(\\phi_{11}, . . . , \\phi_{p1})$ are the loadings of the first principal component; together, the loadings make up the principal component loading vector, $\\phi_1= (\\phi_{11} \\phi_{21} ... \\phi_{p1})^T$\n\nWe constrain the loadings so that their sum of squares is equal to one, since otherwise setting these elements to be arbitrarily large in absolute value could result in an arbitrarily large variance.\n\n## Clustering\n\n### K-Means Clustering\n\nThe K-means clustering method is to partition the data points into k groups such that the sum of squares from points to the assigned cluster center in each group is minimized.\n\n### Hierarchical Clustering\n\nHierarchical clustering is an alternative approach which does not require a pre-specified or a particular choice of $(K)$.\n\nHierarchical Clustering has an advantage that it produces a tree-based representation of the observations: Dendrogram\n\nA dendrogram is built starting from the leaves and combining clusters up to the trunk. The result of hierarchical clustering is a tree-based representation of the objects, which is also known as dendrogram. Observations can be subdivided into groups by cutting the dendrogram at a desired similarity level.\n\n# Hands-on workshop: Principal Component Analysis and Clustering methods\n\n```{r setup, include=FALSE}\nknitr::opts_chunk$set(echo = TRUE)\nrequire(easypackages)\npackages(\"datasets\",\"ISLR\",\"factoextra\",\"tidyverse\",\"RColorBrewer\",\"animation\")\nlibraries(\"datasets\",\"ISLR\",\"factoextra\",\"tidyverse\",\"RColorBrewer\",\"animation\")\n```\n\n**1. Principal Component Analysis (PCA)**\n\n```{r message=FALSE, warning=FALSE}\n## Gentle Machine Learning\n## Principal Component Analysis\n\n\n# Dataset: USArrests is the sample dataset used in \n# McNeil, D. R. (1977) Interactive Data Analysis. New York: Wiley.\n# Murder\tnumeric\tMurder arrests (per 100,000)\n# Assault\tnumeric\tAssault arrests (per 100,000)\n# UrbanPop\tnumeric\tPercent urban population\n# Rape\tnumeric\tRape arrests (per 100,000)\n# For each of the fifty states in the United States, the dataset contains the number \n# of arrests per 100,000 residents for each of three crimes: Assault, Murder, and Rape. \n# UrbanPop is the percent of the population in each state living in urban areas.\nlibrary(datasets)\nlibrary(ISLR)\narrest = USArrests\nstates=row.names(USArrests)\nnames(USArrests)\n\n# Get means and variances of variables\napply(USArrests, 2, mean)\napply(USArrests, 2, var)\n\n# PCA with scaling\npr.out=prcomp(USArrests, scale=TRUE)\nnames(pr.out) # Five\npr.out$center # the centering and scaling used (means)\npr.out$scale # the matrix of variable loadings (eigenvectors)\npr.out$rotation\ndim(pr.out$x)\npr.out$rotation=-pr.out$rotation\npr.out$x=-pr.out$x\nbiplot(pr.out, scale=0)\n\npr.out$sdev\npr.var=pr.out$sdev^2\npr.var\npve=pr.var/sum(pr.var)\npve\nplot(pve, xlab=\"Principal Component\", ylab=\"Proportion of Variance Explained\", ylim=c(0,1),type='b')\nplot(cumsum(pve), xlab=\"Principal Component\", ylab=\"Cumulative Proportion of Variance Explained\", ylim=c(0,1),type='b')\n\n## Use factoextra package\nlibrary(factoextra)\nfviz(pr.out, \"ind\", geom = \"auto\", mean.point = TRUE, font.family = \"Georgia\")\nfviz_pca_biplot(pr.out, font.family = \"Georgia\", col.var=\"firebrick1\")\n```\n\n**2. K-Means Clustering**\n\n```{r message=FALSE, warning=FALSE}\n## Computer purchase example: Animated illustration \n## Adapted from Guru99 tutorial (https://www.guru99.com/r-k-means-clustering.html)\n## Dataset: characteristics of computers purchased.\n## Variables used: RAM size, Harddrive size\n\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(RColorBrewer)\n\ncomputers = read.csv(\"https://raw.githubusercontent.com/guru99-edu/R-Programming/master/computers.csv\") \n\n# Only retain two variables for illustration\nrescaled_comp <- computers[4:5] %>%\n  mutate(hd_scal = scale(hd),\n         ram_scal = scale(ram)) %>%\n  select(c(hd_scal, ram_scal))\n        \nggplot(data = rescaled_comp, aes(x = hd_scal, y = ram_scal)) +\n  geom_point(pch=20, col = \"blue\") + theme_bw() +\n  labs(x = \"Hard drive size (Scaled)\", y =\"RAM size (Scaled)\" ) +\n  theme(text = element_text(family=\"Georgia\")) \n\n# install.packages(\"animation\")\nlibrary(animation)\nset.seed(2345)\nlibrary(animation)\n\n# Animate the K-mean clustering process, cluster no. = 4\nkmeans.ani(rescaled_comp[1:2], centers = 4, pch = 15:18, col = 1:4) \n```\n\n```{r message=FALSE}\n\nsaveGIF(\n  kmeans.ani(rescaled_comp[1:2], centers = 4, pch = 15:18, col = 1:4) ,\n  movie.name = \"kmeans_animated.gif\",\n  img.name = \"kmeans\",\n  convert = \"magick\",\n  cmd.fun,\n  clean = TRUE,\n  extra.opts = \"\"\n)\n```\n\n![animated K-means output](https://datageneration.io/Gentlemachinelearning/images/kmeans_animated.gif){width=\"700\" height=\"500\"}\n\n```{r message=FALSE, warning=FALSE}\n## Iris example\n\n# Without grouping by species\nggplot(iris, aes(Petal.Length, Petal.Width)) + geom_point() + \n  theme_bw() +\n  scale_color_manual(values=c(\"firebrick1\",\"forestgreen\",\"darkblue\"))\n\n# With grouping by species\nggplot(iris, aes(Petal.Length, Petal.Width, color = Species)) + geom_point() + \n  theme_bw() +\n  scale_color_manual(values=c(\"firebrick1\",\"forestgreen\",\"darkblue\"))\n\n# Check k-means clusters\n## Starting with three clusters and 20 initial configurations\nset.seed(20)\nirisCluster <- kmeans(iris[, 3:4], 3, nstart = 20)\nirisCluster\nclass(irisCluster$cluster)\n# Confusion matrix\ntable(irisCluster$cluster, iris$Species)\nirisCluster$cluster <- as.factor(irisCluster$cluster)\nggplot(iris, aes(Petal.Length, Petal.Width, color = irisCluster$cluster)) + geom_point() +\n  scale_color_manual(values=c(\"firebrick1\",\"forestgreen\",\"darkblue\")) +\n  theme_bw()\nactual = ggplot(iris, aes(Petal.Length, Petal.Width, color = Species)) + geom_point() + \n  theme_bw() +\n  scale_color_manual(values=c(\"firebrick1\",\"forestgreen\",\"darkblue\")) +\n  theme(legend.position=\"bottom\") +\n  theme(text = element_text(family=\"Georgia\")) \nkmc = ggplot(iris, aes(Petal.Length, Petal.Width, color = irisCluster$cluster)) + geom_point() +\n  theme_bw() +\n  scale_color_manual(values=c(\"firebrick1\", \"darkblue\", \"forestgreen\")) +\n  theme(legend.position=\"bottom\") +\n  theme(text = element_text(family=\"Georgia\")) \nlibrary(grid)\nlibrary(gridExtra)\ngrid.arrange(arrangeGrob(actual, kmc, ncol=2, widths=c(1,1)), nrow=1)\n```\n\n```{r message=FALSE, warning=FALSE}\n## Wine example\n\n# The wine dataset contains the results of a chemical analysis of wines \n# grown in a specific area of Italy. Three types of wine are represented in the \n# 178 samples, with the results of 13 chemical analyses recorded for each sample. \n# Variables used in this example:\n# Alcohol\n# Malic: Malic acid\n# Ash\n# Source: http://archive.ics.uci.edu/ml/datasets/Wine\n\n# Import wine dataset\nlibrary(readr)\nwine <- read_csv(\"https://raw.githubusercontent.com/datageneration/gentlemachinelearning/master/data/wine.csv\")\n\n\n## Choose and scale variables\nwine_subset <- scale(wine[ , c(2:4)])\n\n## Create cluster using k-means, k = 3, with 25 initial configurations\nwine_cluster <- kmeans(wine_subset, centers = 3,\n                       iter.max = 10,\n                       nstart = 25)\nwine_cluster\n\n# Create a function to compute and plot total within-cluster sum of square (within-ness)\nwssplot <- function(data, nc=15, seed=1234){\n  wss <- (nrow(data)-1)*sum(apply(data,2,var))\n  for (i in 2:nc){\n    set.seed(seed)\n    wss[i] <- sum(kmeans(data, centers=i)$withinss)}\n  plot(1:nc, wss, type=\"b\", xlab=\"Number of Clusters\",\n       ylab=\"Within groups sum of squares\")\n}\n\n# plotting values for each cluster starting from 1 to 9\nwssplot(wine_subset, nc = 9)\n\n# Plot results by dimensions\nwine_cluster$cluster = as.factor(wine_cluster$cluster)\npairs(wine[2:4],\n      col = c(\"firebrick1\", \"darkblue\", \"forestgreen\")[wine_cluster$cluster],\n      pch = c(15:17)[wine_cluster$cluster],\n      main = \"K-Means Clusters: Wine data\")\ntable(wine_cluster$cluster)\n\n## Use the factoextra package to do more\n# install.packages(\"factoextra\")\n\nlibrary(factoextra)\nfviz_nbclust(wine_subset, kmeans, method = \"wss\")\n\n# Use eclust() procedure to do K-Means\nwine.km <- eclust(wine_subset, \"kmeans\", nboot = 2)\n\n# Print result\nwine.km\n\n# Optimal number of clusters using gap statistics\nwine.km$nbclust\nfviz_nbclust(wine_subset, kmeans, method = \"gap_stat\")\n\n# Silhouette plot\nfviz_silhouette(wine.km)\n\nfviz_cluster(wine_cluster, data = wine_subset) + \n  theme_bw() +\n  theme(text = element_text(family=\"Georgia\")) \n\nfviz_cluster(wine_cluster, data = wine_subset, ellipse.type = \"norm\") + \n  theme_bw() +\n  theme(text = element_text(family=\"Georgia\")) \n```\n\n**3. Hierarchical Clustering**\n\n```{r message=FALSE, warning=FALSE}\n\n## Hierarchical Clustering\n## Dataset: USArrests\n#  install.packages(\"cluster\")\narrest.hc <- USArrests %>%\n  scale() %>%                    # Scale all variables\n  dist(method = \"euclidean\") %>% # Euclidean distance for dissimilarity \n  hclust(method = \"ward.D2\")     # Compute hierarchical clustering\n\n# Generate dendrogram using factoextra package\nfviz_dend(arrest.hc, k = 4, # Four groups\n          cex = 0.5, \n          k_colors = c(\"firebrick1\",\"forestgreen\",\"blue\", \"purple\"),\n          color_labels_by_k = TRUE, # color labels by groups\n          rect = TRUE, # Add rectangle (cluster) around groups,\n          main = \"Cluster Dendrogram: USA Arrest data\"\n) + theme(text = element_text(family=\"Georgia\")) \n\n```\n\nPrincipal Component Analysis (PCA) and clustering methods serve distinct purposes in data analysis. PCA focuses on dimensionality reduction, aiming to simplify complex datasets by transforming the original variables into a new set of orthogonal variables called principal components. In contrast, clustering methods aim to uncover natural groupings within the data by partitioning it into clusters of similar data points based on similarity or distance metrics. While PCA seeks to condense the information within the data, clustering methods facilitate the discovery of inherent structures or patterns by grouping data points with shared characteristics.\n\n\n\n\n\n\n\n**References**\n\nJames, Gareth, Daniela Witten, Trevor Hastie, and Robert Tibshirani. 2013 *An introduction to statistical learning*. Vol. 112. New York: Springer.\n","srcMarkdownNoYaml":"\n\n# Unsupervised Learning\n\nUnsupervised learning is a class of machine learning algorithms to identify patterns or grouping structure in the data. Unlike supervised learning which relies on \"supervised\" information such as the dependent variable to guide modeling, unsupervised learning seeks to explore the structure and possible groupings of unlabeled data. This information will be useful to provide pre-processor for supervised learning.\n\nUnsupervised learning has no explicit dependent variable of Y for prediction. Instead, the goal is to discover interesting patterns about the measurements on $(X_{1}), (X_{2}), . . . , (X_{p})$ and identify any subgroups among the observations.\n\nGenerally, in this section, the two general methods are introduced: Principal components analysis and Clustering.\n\n## Principal Component Analysis (PCA)\n\nPrincipal Components Analysis (PCA) produces a low-dimensional representation of a dataset. It finds a sequence of linear combinations of the variables that have maximal variance, and are mutually uncorrelated.\n\nThe first principal component of a set of features $(X_1, X_2, . . . , X_p)$ is the normalized linear combination of the features: <br> $$  Z_1 = \\phi_{11}X_1 +\\phi_{21}X_2 +...+\\phi_{p1}X_p $$ <br>\n\nthat has the largest variance. By normalized, we mean that $\\sum_{j=1}^p\\phi_{j1}^2 = 1$.\n\nThe elements $(\\phi_{11}, . . . , \\phi_{p1})$ are the loadings of the first principal component; together, the loadings make up the principal component loading vector, $\\phi_1= (\\phi_{11} \\phi_{21} ... \\phi_{p1})^T$\n\nWe constrain the loadings so that their sum of squares is equal to one, since otherwise setting these elements to be arbitrarily large in absolute value could result in an arbitrarily large variance.\n\n## Clustering\n\n### K-Means Clustering\n\nThe K-means clustering method is to partition the data points into k groups such that the sum of squares from points to the assigned cluster center in each group is minimized.\n\n### Hierarchical Clustering\n\nHierarchical clustering is an alternative approach which does not require a pre-specified or a particular choice of $(K)$.\n\nHierarchical Clustering has an advantage that it produces a tree-based representation of the observations: Dendrogram\n\nA dendrogram is built starting from the leaves and combining clusters up to the trunk. The result of hierarchical clustering is a tree-based representation of the objects, which is also known as dendrogram. Observations can be subdivided into groups by cutting the dendrogram at a desired similarity level.\n\n# Hands-on workshop: Principal Component Analysis and Clustering methods\n\n```{r setup, include=FALSE}\nknitr::opts_chunk$set(echo = TRUE)\nrequire(easypackages)\npackages(\"datasets\",\"ISLR\",\"factoextra\",\"tidyverse\",\"RColorBrewer\",\"animation\")\nlibraries(\"datasets\",\"ISLR\",\"factoextra\",\"tidyverse\",\"RColorBrewer\",\"animation\")\n```\n\n**1. Principal Component Analysis (PCA)**\n\n```{r message=FALSE, warning=FALSE}\n## Gentle Machine Learning\n## Principal Component Analysis\n\n\n# Dataset: USArrests is the sample dataset used in \n# McNeil, D. R. (1977) Interactive Data Analysis. New York: Wiley.\n# Murder\tnumeric\tMurder arrests (per 100,000)\n# Assault\tnumeric\tAssault arrests (per 100,000)\n# UrbanPop\tnumeric\tPercent urban population\n# Rape\tnumeric\tRape arrests (per 100,000)\n# For each of the fifty states in the United States, the dataset contains the number \n# of arrests per 100,000 residents for each of three crimes: Assault, Murder, and Rape. \n# UrbanPop is the percent of the population in each state living in urban areas.\nlibrary(datasets)\nlibrary(ISLR)\narrest = USArrests\nstates=row.names(USArrests)\nnames(USArrests)\n\n# Get means and variances of variables\napply(USArrests, 2, mean)\napply(USArrests, 2, var)\n\n# PCA with scaling\npr.out=prcomp(USArrests, scale=TRUE)\nnames(pr.out) # Five\npr.out$center # the centering and scaling used (means)\npr.out$scale # the matrix of variable loadings (eigenvectors)\npr.out$rotation\ndim(pr.out$x)\npr.out$rotation=-pr.out$rotation\npr.out$x=-pr.out$x\nbiplot(pr.out, scale=0)\n\npr.out$sdev\npr.var=pr.out$sdev^2\npr.var\npve=pr.var/sum(pr.var)\npve\nplot(pve, xlab=\"Principal Component\", ylab=\"Proportion of Variance Explained\", ylim=c(0,1),type='b')\nplot(cumsum(pve), xlab=\"Principal Component\", ylab=\"Cumulative Proportion of Variance Explained\", ylim=c(0,1),type='b')\n\n## Use factoextra package\nlibrary(factoextra)\nfviz(pr.out, \"ind\", geom = \"auto\", mean.point = TRUE, font.family = \"Georgia\")\nfviz_pca_biplot(pr.out, font.family = \"Georgia\", col.var=\"firebrick1\")\n```\n\n**2. K-Means Clustering**\n\n```{r message=FALSE, warning=FALSE}\n## Computer purchase example: Animated illustration \n## Adapted from Guru99 tutorial (https://www.guru99.com/r-k-means-clustering.html)\n## Dataset: characteristics of computers purchased.\n## Variables used: RAM size, Harddrive size\n\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(RColorBrewer)\n\ncomputers = read.csv(\"https://raw.githubusercontent.com/guru99-edu/R-Programming/master/computers.csv\") \n\n# Only retain two variables for illustration\nrescaled_comp <- computers[4:5] %>%\n  mutate(hd_scal = scale(hd),\n         ram_scal = scale(ram)) %>%\n  select(c(hd_scal, ram_scal))\n        \nggplot(data = rescaled_comp, aes(x = hd_scal, y = ram_scal)) +\n  geom_point(pch=20, col = \"blue\") + theme_bw() +\n  labs(x = \"Hard drive size (Scaled)\", y =\"RAM size (Scaled)\" ) +\n  theme(text = element_text(family=\"Georgia\")) \n\n# install.packages(\"animation\")\nlibrary(animation)\nset.seed(2345)\nlibrary(animation)\n\n# Animate the K-mean clustering process, cluster no. = 4\nkmeans.ani(rescaled_comp[1:2], centers = 4, pch = 15:18, col = 1:4) \n```\n\n```{r message=FALSE}\n\nsaveGIF(\n  kmeans.ani(rescaled_comp[1:2], centers = 4, pch = 15:18, col = 1:4) ,\n  movie.name = \"kmeans_animated.gif\",\n  img.name = \"kmeans\",\n  convert = \"magick\",\n  cmd.fun,\n  clean = TRUE,\n  extra.opts = \"\"\n)\n```\n\n![animated K-means output](https://datageneration.io/Gentlemachinelearning/images/kmeans_animated.gif){width=\"700\" height=\"500\"}\n\n```{r message=FALSE, warning=FALSE}\n## Iris example\n\n# Without grouping by species\nggplot(iris, aes(Petal.Length, Petal.Width)) + geom_point() + \n  theme_bw() +\n  scale_color_manual(values=c(\"firebrick1\",\"forestgreen\",\"darkblue\"))\n\n# With grouping by species\nggplot(iris, aes(Petal.Length, Petal.Width, color = Species)) + geom_point() + \n  theme_bw() +\n  scale_color_manual(values=c(\"firebrick1\",\"forestgreen\",\"darkblue\"))\n\n# Check k-means clusters\n## Starting with three clusters and 20 initial configurations\nset.seed(20)\nirisCluster <- kmeans(iris[, 3:4], 3, nstart = 20)\nirisCluster\nclass(irisCluster$cluster)\n# Confusion matrix\ntable(irisCluster$cluster, iris$Species)\nirisCluster$cluster <- as.factor(irisCluster$cluster)\nggplot(iris, aes(Petal.Length, Petal.Width, color = irisCluster$cluster)) + geom_point() +\n  scale_color_manual(values=c(\"firebrick1\",\"forestgreen\",\"darkblue\")) +\n  theme_bw()\nactual = ggplot(iris, aes(Petal.Length, Petal.Width, color = Species)) + geom_point() + \n  theme_bw() +\n  scale_color_manual(values=c(\"firebrick1\",\"forestgreen\",\"darkblue\")) +\n  theme(legend.position=\"bottom\") +\n  theme(text = element_text(family=\"Georgia\")) \nkmc = ggplot(iris, aes(Petal.Length, Petal.Width, color = irisCluster$cluster)) + geom_point() +\n  theme_bw() +\n  scale_color_manual(values=c(\"firebrick1\", \"darkblue\", \"forestgreen\")) +\n  theme(legend.position=\"bottom\") +\n  theme(text = element_text(family=\"Georgia\")) \nlibrary(grid)\nlibrary(gridExtra)\ngrid.arrange(arrangeGrob(actual, kmc, ncol=2, widths=c(1,1)), nrow=1)\n```\n\n```{r message=FALSE, warning=FALSE}\n## Wine example\n\n# The wine dataset contains the results of a chemical analysis of wines \n# grown in a specific area of Italy. Three types of wine are represented in the \n# 178 samples, with the results of 13 chemical analyses recorded for each sample. \n# Variables used in this example:\n# Alcohol\n# Malic: Malic acid\n# Ash\n# Source: http://archive.ics.uci.edu/ml/datasets/Wine\n\n# Import wine dataset\nlibrary(readr)\nwine <- read_csv(\"https://raw.githubusercontent.com/datageneration/gentlemachinelearning/master/data/wine.csv\")\n\n\n## Choose and scale variables\nwine_subset <- scale(wine[ , c(2:4)])\n\n## Create cluster using k-means, k = 3, with 25 initial configurations\nwine_cluster <- kmeans(wine_subset, centers = 3,\n                       iter.max = 10,\n                       nstart = 25)\nwine_cluster\n\n# Create a function to compute and plot total within-cluster sum of square (within-ness)\nwssplot <- function(data, nc=15, seed=1234){\n  wss <- (nrow(data)-1)*sum(apply(data,2,var))\n  for (i in 2:nc){\n    set.seed(seed)\n    wss[i] <- sum(kmeans(data, centers=i)$withinss)}\n  plot(1:nc, wss, type=\"b\", xlab=\"Number of Clusters\",\n       ylab=\"Within groups sum of squares\")\n}\n\n# plotting values for each cluster starting from 1 to 9\nwssplot(wine_subset, nc = 9)\n\n# Plot results by dimensions\nwine_cluster$cluster = as.factor(wine_cluster$cluster)\npairs(wine[2:4],\n      col = c(\"firebrick1\", \"darkblue\", \"forestgreen\")[wine_cluster$cluster],\n      pch = c(15:17)[wine_cluster$cluster],\n      main = \"K-Means Clusters: Wine data\")\ntable(wine_cluster$cluster)\n\n## Use the factoextra package to do more\n# install.packages(\"factoextra\")\n\nlibrary(factoextra)\nfviz_nbclust(wine_subset, kmeans, method = \"wss\")\n\n# Use eclust() procedure to do K-Means\nwine.km <- eclust(wine_subset, \"kmeans\", nboot = 2)\n\n# Print result\nwine.km\n\n# Optimal number of clusters using gap statistics\nwine.km$nbclust\nfviz_nbclust(wine_subset, kmeans, method = \"gap_stat\")\n\n# Silhouette plot\nfviz_silhouette(wine.km)\n\nfviz_cluster(wine_cluster, data = wine_subset) + \n  theme_bw() +\n  theme(text = element_text(family=\"Georgia\")) \n\nfviz_cluster(wine_cluster, data = wine_subset, ellipse.type = \"norm\") + \n  theme_bw() +\n  theme(text = element_text(family=\"Georgia\")) \n```\n\n**3. Hierarchical Clustering**\n\n```{r message=FALSE, warning=FALSE}\n\n## Hierarchical Clustering\n## Dataset: USArrests\n#  install.packages(\"cluster\")\narrest.hc <- USArrests %>%\n  scale() %>%                    # Scale all variables\n  dist(method = \"euclidean\") %>% # Euclidean distance for dissimilarity \n  hclust(method = \"ward.D2\")     # Compute hierarchical clustering\n\n# Generate dendrogram using factoextra package\nfviz_dend(arrest.hc, k = 4, # Four groups\n          cex = 0.5, \n          k_colors = c(\"firebrick1\",\"forestgreen\",\"blue\", \"purple\"),\n          color_labels_by_k = TRUE, # color labels by groups\n          rect = TRUE, # Add rectangle (cluster) around groups,\n          main = \"Cluster Dendrogram: USA Arrest data\"\n) + theme(text = element_text(family=\"Georgia\")) \n\n```\n\nPrincipal Component Analysis (PCA) and clustering methods serve distinct purposes in data analysis. PCA focuses on dimensionality reduction, aiming to simplify complex datasets by transforming the original variables into a new set of orthogonal variables called principal components. In contrast, clustering methods aim to uncover natural groupings within the data by partitioning it into clusters of similar data points based on similarity or distance metrics. While PCA seeks to condense the information within the data, clustering methods facilitate the discovery of inherent structures or patterns by grouping data points with shared characteristics.\n\n\n\n\n\n\n\n**References**\n\nJames, Gareth, Daniela Witten, Trevor Hastie, and Robert Tibshirani. 2013 *An introduction to statistical learning*. Vol. 112. New York: Springer.\n"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":{"html_document":{"toc":"yes","toc_float":"yes","highlight":"default","theme":"libera","number_sections":"yes"}},"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"knitr"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","css":["../styles.css"],"toc":true,"output-file":"lab04.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","appendix-view-license":"View License","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words","listing-page-filter":"Filter","draft":"Draft"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.5.57","editor":"visual","theme":"zephyr","title":"Knowledge Mining: Lab04 R programming (Unsupervised learning)","author":"Guan Chen","date":"last-modified","title-block-banner":true},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}