{
  "hash": "2734610a5500a351ce46036dca54c067",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Knowledge Mining (EPPS 6323)\"\nsubtitle: \"Assignment 7\"\nauthor: \"Guan Chen\"\ndate: last-modified\ntoc: false\ntitle-block-banner: true\n---\n\n::: {.cell}\n\n```{.r .cell-code}\nrequire(ISLR)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nLoading required package: ISLR\n```\n\n\n:::\n\n```{.r .cell-code}\n# Check dataset Smarket\n?Smarket\nnames(Smarket)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"Year\"      \"Lag1\"      \"Lag2\"      \"Lag3\"      \"Lag4\"      \"Lag5\"     \n[7] \"Volume\"    \"Today\"     \"Direction\"\n```\n\n\n:::\n\n```{.r .cell-code}\nsummary(Smarket)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n      Year           Lag1                Lag2                Lag3          \n Min.   :2001   Min.   :-4.922000   Min.   :-4.922000   Min.   :-4.922000  \n 1st Qu.:2002   1st Qu.:-0.639500   1st Qu.:-0.639500   1st Qu.:-0.640000  \n Median :2003   Median : 0.039000   Median : 0.039000   Median : 0.038500  \n Mean   :2003   Mean   : 0.003834   Mean   : 0.003919   Mean   : 0.001716  \n 3rd Qu.:2004   3rd Qu.: 0.596750   3rd Qu.: 0.596750   3rd Qu.: 0.596750  \n Max.   :2005   Max.   : 5.733000   Max.   : 5.733000   Max.   : 5.733000  \n      Lag4                Lag5              Volume           Today          \n Min.   :-4.922000   Min.   :-4.92200   Min.   :0.3561   Min.   :-4.922000  \n 1st Qu.:-0.640000   1st Qu.:-0.64000   1st Qu.:1.2574   1st Qu.:-0.639500  \n Median : 0.038500   Median : 0.03850   Median :1.4229   Median : 0.038500  \n Mean   : 0.001636   Mean   : 0.00561   Mean   :1.4783   Mean   : 0.003138  \n 3rd Qu.: 0.596750   3rd Qu.: 0.59700   3rd Qu.:1.6417   3rd Qu.: 0.596750  \n Max.   : 5.733000   Max.   : 5.73300   Max.   :3.1525   Max.   : 5.733000  \n Direction \n Down:602  \n Up  :648  \n           \n           \n           \n           \n```\n\n\n:::\n\n```{.r .cell-code}\n# Create a dataframe for data browsing\nsm=Smarket\n\n# Bivariate Plot of inter-lag correlations\npairs(Smarket,col=Smarket$Direction,cex=.5, pch=20)\n```\n\n::: {.cell-output-display}\n![](assignment7_files/figure-html/unnamed-chunk-1-1.png){width=672}\n:::\n\n```{.r .cell-code}\n# Logistic regression\nglm.fit=glm(Direction~Lag1+Lag2+Lag3+Lag4+Lag5+Volume,\n            data=Smarket,family=binomial)\nsummary(glm.fit)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nglm(formula = Direction ~ Lag1 + Lag2 + Lag3 + Lag4 + Lag5 + \n    Volume, family = binomial, data = Smarket)\n\nCoefficients:\n             Estimate Std. Error z value Pr(>|z|)\n(Intercept) -0.126000   0.240736  -0.523    0.601\nLag1        -0.073074   0.050167  -1.457    0.145\nLag2        -0.042301   0.050086  -0.845    0.398\nLag3         0.011085   0.049939   0.222    0.824\nLag4         0.009359   0.049974   0.187    0.851\nLag5         0.010313   0.049511   0.208    0.835\nVolume       0.135441   0.158360   0.855    0.392\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 1731.2  on 1249  degrees of freedom\nResidual deviance: 1727.6  on 1243  degrees of freedom\nAIC: 1741.6\n\nNumber of Fisher Scoring iterations: 3\n```\n\n\n:::\n\n```{.r .cell-code}\nglm.probs=predict(glm.fit,type=\"response\") \nglm.probs[1:5]\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n        1         2         3         4         5 \n0.5070841 0.4814679 0.4811388 0.5152224 0.5107812 \n```\n\n\n:::\n\n```{.r .cell-code}\nglm.pred=ifelse(glm.probs>0.5,\"Up\",\"Down\")\nattach(Smarket)\ntable(glm.pred,Direction)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n        Direction\nglm.pred Down  Up\n    Down  145 141\n    Up    457 507\n```\n\n\n:::\n\n```{.r .cell-code}\nmean(glm.pred==Direction)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.5216\n```\n\n\n:::\n\n```{.r .cell-code}\n# Make training and test set for prediction\ntrain = Year<2005\nglm.fit=glm(Direction~Lag1+Lag2+Lag3+Lag4+Lag5+Volume,\n            data=Smarket,family=binomial, subset=train)\nglm.probs=predict(glm.fit,newdata=Smarket[!train,],type=\"response\") \nglm.pred=ifelse(glm.probs >0.5,\"Up\",\"Down\")\nDirection.2005=Smarket$Direction[!train]\ntable(glm.pred,Direction.2005)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n        Direction.2005\nglm.pred Down Up\n    Down   77 97\n    Up     34 44\n```\n\n\n:::\n\n```{.r .cell-code}\nmean(glm.pred==Direction.2005)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.4801587\n```\n\n\n:::\n\n```{.r .cell-code}\n#Fit smaller model\nglm.fit=glm(Direction~Lag1+Lag2,\n            data=Smarket,family=binomial, subset=train)\nglm.probs=predict(glm.fit,newdata=Smarket[!train,],type=\"response\") \nglm.pred=ifelse(glm.probs >0.5,\"Up\",\"Down\")\ntable(glm.pred,Direction.2005)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n        Direction.2005\nglm.pred Down  Up\n    Down   35  35\n    Up     76 106\n```\n\n\n:::\n\n```{.r .cell-code}\nmean(glm.pred==Direction.2005)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.5595238\n```\n\n\n:::\n\n```{.r .cell-code}\n# Check accuracy rate\n106/(76+106)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.5824176\n```\n\n\n:::\n\n```{.r .cell-code}\n# Can you interpret the results?\n```\n:::\n\n\n2a: Predictor variables are normally distributed, each predictor variable has the same variance, and the response variable must be categorical as LDA is used for classification problems.\n\n2b: LDA is used for multiclass classification whereas logistic regression is for binary classification. LDA also makes assumptions about the distribution of the data whereas logistic regression does not.\n\n2c: The receiving operator characteristic (ROC) evaluates the performance of a classifier model.\n\n2d: Sensitivity is the true positive rate. Specificity is the true negative rate. Generally, sensitivity is more important since there tends to be immediate and severe consequences for false positives (i.e. an innocent person who's found guilty will go to jail) rather than false negatives. Neither type I or II errors are desirable, but high sensitivity should be prioritized. \n\n2e: Sensitivity is more important for prediction based on the chart.\n\n3: \n\n::: {.cell}\n\n```{.r .cell-code}\nprediction_error <- (252+23)/10000\ncat(\"Prediction error:\", formatC(prediction_error * 100, format = \"f\", digits = 2), \"%\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nPrediction error: 2.75 %\n```\n\n\n:::\n:::\n",
    "supporting": [
      "assignment7_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}